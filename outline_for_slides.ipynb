{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Outline for LDA Lecture @MSU ML Club\n",
    "- Friday 11am, Oct 12\n",
    "- It is the meeting room in the offices. AES 200k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Slide 1\n",
    "- Title: Discovering Topics in a Corpus using Latent Dirichlet Allocation\n",
    "![process high level view](images/pipeline.png)\n",
    "![logos](images/logos.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "- Hello, my name is Tovio Roberts and I'm here to talk about Latent Dirichlet Allocation, known as the other LDA, not to be confused with [Linear Discriminant Analysis](https://en.wikipedia.org/wiki/Linear_discriminant_analysis). \n",
    "\n",
    "- Before we get into LDA, \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Slide 2: About me:\n",
    "- Tovio Roberts\n",
    "    - Former Arabic major @Wayne State, Detroit\n",
    "        - lots of language study in a variety of languages\n",
    "    - Former CompSci mentor with cognitively diverse college students\n",
    "    - Galvanize DSI 64 alumnus\n",
    "    - Lead Technical Admissions Officer for the Data Science Immersive at Galvanize\n",
    "    - Data Specialist at Carbon Collective\n",
    "    - Former CompSci major @MSU\n",
    "        - Currently IDP in Machine Learning @MSU, ~Spring 2019?\n",
    "    - Exploring Masters programs in Computational Linguistics\n",
    "    \n",
    "- github/clownfragment\n",
    "- dataisartisdata.com\n",
    "- tovioroberts@gmail.com\n",
    "- linkedin.com/in/Tovio-Roberts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "- _A little bit about me..._\n",
    "\n",
    "- _I've studied languages quite a bit, former Arabic major._\n",
    "\n",
    "- _I currently work as the lead technical admissions officer for the Data Science Immersive at Galvanize and as the data specialist (whatever that means) for a small startup called Carbon Collective working on gamifying carbon footprint reduction._\n",
    "\n",
    "- _I attended the 3 month Data Science Immersive at Galvanize last Spring._\n",
    "\n",
    "- _I worked for about 5 years as a CompSci tutor and mentor with Cognitively Diverse college students, mostly with ASD, in a one on one format, assisting with executive function, designing specialized curriculums and study plans._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Slide 3: First, Let's Frame a Use Case\n",
    "![Consolidate Documents from different Departments](images/departments_to_corpus.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "- _So let's motivate LDA through a context._\n",
    "\n",
    "- _In a university, there are many departments. Let's say MSU, wants to digitize their study resources across all departments and create a document recommender system that will suggest materials based on documents similar to what a given student is studying. The administration plans on pooling thousands of digital documents into a single corpus._\n",
    "\n",
    "- _Notice here that the concept of a \"subject\" is defined by humans. Potentially, the documents could be given tags to facilitate searching. However, we're hoping to automate the process of document suggestion with little human-intervention._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Slide 4: Subjects vs Topics\n",
    "- **Subject**: human-projected category, ie., History, Biology, Data Science\n",
    "- **Topic**: a latent signal discerned within a corpus, often described by a list of words/stems that \"should make sense\" to a human observer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "- _I am going to posit that, in order to deliver all appropriate study materials from many different departments, we should abandon the human-projected subject associated with the document. We might want to do this because we care primarily about the content of the document.  For example, if we're discussing formal languages, we want to make sure that content from the Linguistics department that is similar to content from the Computer Science department, such as those describing the Chomsky hierarchy of formal grammars, be related regardless._\n",
    "\n",
    "- _We want the machine to do the work of discovering the distribution of latent topics in a given document._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Slide 5: What do We Feed to the Machine?\n",
    "![truckasaurus](images/truckasaurus.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "- _What do we feed to our machine?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Slide 6: Term Frequency (TF) Matrix\n",
    "![bag of words image](images/bow.png)\n",
    "- aka, Bag of Words/Term Frequency\n",
    "    - order of words in the document is not relevant\n",
    "    - sparse matrix \n",
    "- Requires an NLP cleaning pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "- _Theoretically, Latent Dirichlet Allocation requires a Term Frequency Matrix, though I've used TFIDF matrices with similar results to TF matrices. Keep in mind that the input matrix is rather wide, the length of a row being the length of the vocabulary of the entire corpus. In other words, there's a column for every distinct word found in every document in the input._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Slide 7: TF Matrix / Bag of Words\n",
    "\n",
    "| doc index | word0 | word1 | word2 | word3 |... | wordn\n",
    "|-----------|-------|-------|-------|-------|----|------\n",
    "|doc0       | 0     | 1     | 0     |  2    |... | 1\n",
    "|doc1       | 0     | 0     | 1     |  0    |... | 0 \n",
    "|doc2       | 1     | 2     | 0     |  0    |... | 0\n",
    "|doc3       | 1     | 0     | 0     |  1    |... | 1\n",
    "|...        | ...   | ...   | ...   | ...   |... | ...\n",
    "|docn       | 0     | 2     | 0     |  0    |... | 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "_So, every document in our input is represented by a row, and has a frequency value for every word in the vocabulary. \n",
    "Realistically , a given document will not contain most of the words in the vocabulary, thus this is a sparse matrix._\n",
    "\n",
    "_We arrive at a usable TF Matrix through constructing a Cleaning Pipeline. To give a sense of what this pipeline is doing, let's look at some real documents._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Slide 8: Let's Build a Corpus\n",
    "- Sample of 36000 documents (Anki flash cards) from 3 Human-Defined Subjects\n",
    "    - 12000 from History\n",
    "    - 12000 from Data Science/Statistics\n",
    "    - 12000 from Biology\n",
    "- Using 1-grams, for sake of dimensionality, though n-grams are possible\n",
    "- Remove words most/least common across documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "_As a proof of concept, let's build a corpus where the documents come from 3 distinct, human-defined subjects, History, Data Science, and Biology. I used flash card decks contributed to ankiweb_\n",
    "\n",
    "_We can imagine that a topic-modeling algorithm searching for 3 topics will likely distinguish three topics represented by words that seem to satisfy the three subjects._\n",
    "\n",
    "_I'm using 1-grams here for \"words,\" but there's no reason that you couldn't use any n-gram representation, it just massively increases the dimensions of the sparse matrix and wasn't at all necessary given the subject delineation of this corpus._\n",
    "\n",
    "_One last adjustment that I commit is to remove the words that appear in more than a certain percentage of documents, as well as words that only appear in less than a certain number of documents. This is similar to the automatic weighting you see in Term Frequency, Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Slide 9: Input Must be Cleaned\n",
    "![uncleaned input data](images/wordmap_data_uncleaned.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "_Here we see a wordcloud from just the Data Science cards. We can see a number of artifacts from html, javascript and LaTEX. My pipeline includes functions to strip code, numeric characters and LaTEX. I will likely rewrite my cleaning functions to keep the LaTEX in a translated form, as some cards only have LaTEX and the word 'derivative' would likely contribute to a document's topic distribution._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Slide 10: Features (words) should be Canonical\n",
    "- Stemming: reduce to a \"root\" form\n",
    "    - Eliminates tense\n",
    "- Could instead lemmatize for potentially more readable root word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "_In general when we use a TF or TFIDF matrix, we want to stem the words. Stemming strips different forms of a given word, such as the words \"complex\" and \"complexity\", to a single canonical form._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Slide 11: Cleaned/Stemmed Document\n",
    "![document before and after](images/doc_before_after.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "_This is a single document, before and after cleaning. The underlined words illustrate stemming, html has been removed, and everything is lowercased._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Slide 12: Data Science Cards, Cleaned and Stemmed\n",
    "![data science wordcloud](images/data_science_wordcloud.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "_Here we see a wordcloud of the cleaned and stemmed vocabulary from the DataScience flashcard deck. The larger the word, the more frequently it occurs in the corpus._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Slide 13: Biology Cards, Cleaned and Stemmed\n",
    "![biology wordcloud](images/biology_wordcloud.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "- _And the same for Biology_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Slide 14: History Cards, Cleaned and Stemmed\n",
    "![history wordcloud](images/history_wordcloud.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "_And History_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Slide 15: Cleaned Corpora Combined\n",
    "\n",
    "![Full cleaned corpus](images/full_corpus_cleaned.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "_And here we see a wordcloud for the entire corpus. We can see some remnants of markup and code, but I think we're doing ok._\n",
    "\n",
    "_I just want to take a moment and say that Data Cleaning takes a long time. In natural language processing, especially when using a bag of words or TFIDF representation of documents, cleaning is key as it provides better consistency and reduces dimensionality of your input._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Slide 16: Some Selected Word Counts\n",
    "![some selected word counts](images/some_selected_word_counts.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "_If we look at some counts of words from the different decks, we can get an idea of word relevance or salience to a given subject. It's very telling that the word \"distribution\" occurs 800 times in the Data Science deck, and less than 40 times in each of the other two decks. The word \"cell\" does not occur in History, and \"war\" only occurs in \"History\"._\n",
    "\n",
    "_Intuitively, we can start think of modeling topics as a process involving observation of word occurrences in documents as well relative to word occurences within the corpus._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Slide 17: What is Topic Modeling?\n",
    "- Unsupervised Learning method for finding latent semantic structures in a text or corpus\n",
    "- Common to assume that a given document is primarily \"about\" a given topic\n",
    "- Common to assume that a document clustered into a given topic will have a high occurrence of topic-salient words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "_That intuition is justified. Topic Modeling is the field within unsupervised learning dedicated to finding latent semantic structures within text and within corpora._\n",
    "\n",
    "_For our purposes, let's assume that a given document is primarily about a given topic and that that document will contain a high frequency of words representative of that topic._\n",
    "\n",
    "_Now that we have a prepared corpus and an understanding that our goal is to discern latent semantic structures in the corpus, we can more easily describe LDA._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Slide 18: Latent Dirichlet Allocation\n",
    "- 2002: David Blei, Andrew Ng, and Michael I. Jordan\n",
    "- \"LDA yields list of salient groups of related words over a collection, with documents represented as a mixture of these groups.\"\n",
    "    - A document is a probability distribution over topics\n",
    "    - A topic is a probability distribution over words\n",
    "- More Intuitive Interpretation than SVD or NMF\n",
    "    - ie., doc0 = {topic1: 50%, topic2: 25%,  topic3 : 25%}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "_Latent Dirichlet Allocation is one of the most common topic modeling algorithms, alongside more matrix-based techniques such as Singular Value Decomposition and Non-Negative Matrix Factorization._\n",
    "\n",
    "_\"LDA is capable of yielding a list of salient groups of related words, aka 'topics', over a collection, with documents represented as a mixture of these groups or topics.\"_\n",
    "\n",
    "_We can think of a document as a probability distribution over topics, and a topic as a probability distribution over words. Notice here, again, that we will assume that documents with similar topics will use similar groups of words. **Latent topics are discovered by discovering groups of words in the corpus that frequently occur together within documents**_ \n",
    "\n",
    "_What really discern LDA from other bag of words models, like Non-negative Matrix Factorization or Singular Value Decomposition, is that we're not really focused on the term frequencies, we are concerned with the distribution of words across topics._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Slide 18: LDA, a Simple Metaphor\n",
    "- you want to figure out peoples' (words') interests (topics) in a city based on where they hang out\n",
    "- assume that people go to specific places based on interests they share with other people who go to those places\n",
    "- assume we can associate people with interests and places with interests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "_Let's say you move to a new city and you want to figure out people's interests based on where they hang out. If you assume people go to specific places based on interests they share with other people, then we can assume that interests are represented by a cross section of people and that places draw certain people based on being intersections of interests._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Slide 19: ... Walking around town\n",
    "\n",
    "1. Choose a number of interests/topics, k=3\n",
    "2. Guess as to what people are indicative of the given interests\n",
    "3. Guess the interests that draw a specific person to a specific place\n",
    "4. Over and over again\n",
    "    - For each place and person\n",
    "        - update likelihood of the guesses based on other people in that space\n",
    " \n",
    "- The guesses will get better \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "_You decide that there are 3 interests people may have that would lead them to a certain place._\n",
    "\n",
    "_So you take a first walk around town and record guesses as to which people represent certain interests, and you guess percentages of the 3 interests that might draw people to a given place._\n",
    "\n",
    "_Since you're new in town, you're very likely to make a lot of bad guesses. However, you're planning on repeating your walk every day for a few years, updating the guesses in your notebook on every walk._\n",
    "\n",
    "_You'll start recognizing that certain people appear in certain places, in the company of other people with similar interests. Eventually, you find that you can describe interests in terms of the people are into those interests._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Slide 20: LDA, Restated\n",
    "[source](http://obphio.us/pdfs/lda_tutorial.pdf)\n",
    "- Given a Corpus of documents, LDA learns the topic representation in each document and the word distribution of each topic\n",
    "- LDA identifies the topics that are likely to have generated the corpus \n",
    "- uses a sparse Dirichlet prior:\n",
    "    - samples from a simplex so the k topics add to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "_So let's restate LDA really quick then describe a less metaphorical view of the LDA procedure._\n",
    "\n",
    "_Given a corpus of documents, LDA learns the topic representation in each document and the word distribution of each topic. The assumption is that a distribution of topics is used to generate each document within the corpus._\n",
    "\n",
    "_We use a sparse Dirichlet prior to seed the LDA model on it's initial random assignments of each word in a given document to each of the topics, as well as the mix of topic probabilities for each document in the corpus._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "# Slide 21: LDA, High-Level View\n",
    "[For more in-depth view of LDA, see the accompanying notebook](https://github.com/clownfragment/Discovering-Topics-in-a-Corpus-using-Latent-Dirichlet-Allocation/blob/master/lda_notes.ipynb)\n",
    "1. Randomly assign each word in each document to one of the _k_ topics\n",
    "2. For each document $d$:\n",
    "    - Assume all topic assignments except for current one to be correct\n",
    "    - Calculate 2 proportions\n",
    "        1. Proportion of words in document $d$ that are currently assigned to topic $t = p(\\text{topic } t \\mid \\text{document }d)$\n",
    "        2. Proportion of assignments to topic $t$ over all documents that came from this word $w = p(\\text{word }w \\mid \\text{topic }t)$\n",
    "    - multiply the proportions and assign $w$ a new topic based on that probability.\n",
    "        - $p(\\text{topic } t \\mid \\text{document }d) \\times p(\\text{word } w \\mid \\text{topic }t)$\n",
    "            - probability that topic $t$ generated word $w$ in a given document\n",
    "3. Reach steady state and return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "_So here is a slightly deeper way to understand what LDA is doing. If you look at the github repo for this presentation, there's more in-depth material there, along with links to further explanations if you want to go even deeper._\n",
    "\n",
    "_Let's walk through this really quick before taking a look at some results_\n",
    "\n",
    "_Initially, we randomly assign each word in each document to a topic. We then look at each document in turn and calculate the proportions of words in that document that are currently assigned to a given topic. We also calculate the proportion of assignments of that topic over all documents that have that word. We multiply those proportions and assign a new topic to the document based on that product._\n",
    "\n",
    "_We do this over and over again for every document, either for a maximum number of epochs or until we don't see a significant update in the probability distributions._\n",
    "\n",
    "_This is effectively the Expectation Maximization algorithm, but using the Dirichlet distribution instead of a Gaussian. I have some pseudocode for the EM on the repo as well._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "# Slide 22: Results on the Flashcard Corpus\n",
    "[Repo for LDA on flashcard corpus](https://github.com/clownfragment/a-smarter-flashcard)\n",
    "\n",
    "* [Interactive LDA on Term Freq Matrix of Full Corpus](https://rawgit.com/clownfragment/a-smarter-flashcard/master/images/all_count_vect_topics.html)\n",
    "* [Interactive LDA on TF-IDF Matrix of Full Corpus](https://rawgit.com/clownfragment/a-smarter-flashcard/master/images/all_tfidf_vect_topics.html)\n",
    "* [Interactive LDA on Term Freq Matrix of Biology Subcorpus](https://rawgit.com/clownfragment/a-smarter-flashcard/master/images/bio_count_vect_topics.html)\n",
    "* [Interactive LDA on TF-IDF Matrix of Biology Subcorpus](https://rawgit.com/clownfragment/a-smarter-flashcard/master/images/bio_tfidf_vect_topics.html)\n",
    "* [Interactive LDA on Term Freq Matrix of Data Science Subcorpus](https://rawgit.com/clownfragment/a-smarter-flashcard/master/images/datascience_count_vect_topics.html)\n",
    "* [Interactive LDA on TF-IDF Matrix of Data Science Subcorpus](https://rawgit.com/clownfragment/a-smarter-flashcard/master/images/datascience_tfidf_vect_topics.html)\n",
    "* [Interactive LDA on Term Freq Matrix of History Subcorpus](https://rawgit.com/clownfragment/a-smarter-flashcard/master/images/his_count_vect_topics.html)\n",
    "* [Interactive LDA on TF-IDF Matrix of History Subcorpus](https://rawgit.com/clownfragment/a-smarter-flashcard/master/images/his_tfidf_vect_topics.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "_So let's look at some results of the LDA model. I used the Gensim version instead of the sklearn version of LDA. To visualize the topic distributions, I'm using a pretty amazing tool called PyLDAvis._\n",
    "\n",
    "**click on Term Freq of Full Corpus**\n",
    "\n",
    "_The Left side of the Plot is constructed using MDS (MultiDimensional Scaling) where PCA is projecting to a 2-dimensional space, and Jensen-Shannon divergence gives distance measurement between topic distributions of words. The x and y axes don't mean much here aside from framing those relative distances._ \n",
    "\n",
    "_The size of the topic bubbles indicates the prevalence of a given topic, which is not the number of documents in a given topic cluster, but the occurrence of a given topic across the entire corpus._\n",
    "\n",
    "\n",
    "**Click on Topics 1, 2 and 3**\n",
    "\n",
    "_Let's look at each topic in turn and look at words in each topic ranked by their topic-specific probability, or $\\lambda$ set to 1. Here, $\\lambda$, controls a log likelihood weighting between term probabilities within the topic and the discernment power of terms within the topic, also known as the lift. By setting $\\lambda$ to one, we are seeing words that have high occurrence in documents when the mix of that topic is high._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "# Slide 23: What does Topic 1 Reflect?\n",
    "![data science terms](images/data_science_terms.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "_So we can make a guess as to what subject topic 1 seems to be reflecting. Remember, we pooled three subjects, Data Science, Biology and History. The high occurrence words for topic 1 are data, model, variable, value, distribution, feature... what does that sound like?  Data Science_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "# Slide 24: What does Topic 2 Reflect?\n",
    "![history terms](images/history_terms.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "_Topic 2 has high occurrence words like state, war, empire, people, and power.  What does that sound like?  History.  Notice here that the word \"amask\" is high-occurring as well. That is a formatting remnant that was not removed during cleaning, as is the word \"style\" which occurs a little further down._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "# Slide 25: What does Topic 3 Reflect?\n",
    "![biology terms](images/biology_terms.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "_Topic 3 is pretty clearly representative of cards from the Biology flashcard set._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "# Slide 26: What if our Corpus is Not So Delineated?\n",
    "* [Interactive LDA on Term Freq Matrix of Biology Subcorpus](https://rawgit.com/clownfragment/a-smarter-flashcard/master/images/bio_count_vect_topics.html)\n",
    "    - What is happening with Topic 5?\n",
    "    - Notice the crossover between Topic 1 and Topic 3\n",
    "* Interpreting these Topics:\n",
    "    - Topic 5: code remnants\n",
    "    - Topics 1 & 3: cellular biology?\n",
    "    - Topic 2: pathology on the cellular level?\n",
    "    - Topic 4: disease and diagnosis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "_So it's not a big surprise that a corpus of such distinct subjects as Biology, History and Data Science would so clearly segment into three topics. That's a bit of a tautology and really provides some confidence that LDA is working_\n",
    "\n",
    "**click on link to Bio deck**\n",
    "\n",
    "_To get a more realistic sense of LDA, let's look at just one of the flashcard sets, the Biology deck. In this case, I set the number of topics to 5, so k=5. There were 12000 documents as input._\n",
    "\n",
    "_Remember when I mentioned that there were some markdown and code tags that I did not effectively remove from the biology deck? If we look at Topic 5, it's clear that Topic 5 is represented by words from javascript and document formatting. I left this in because I wanted to point out that another use case for LDA is as an informant for data cleaning and other non-semantic NLP applications._\n",
    "\n",
    "_Topics 1 and 3 have a lot of overlap. It's possible to imagine that the Jensen-Shannon distance between their word distributions is low and that if we only built the model with 4 topics, It seems likely that the resulting word distribution would look like a fusion of Topics 1 and 3. However, LDA does not explicitly capture correlations between topics, and here we're only looking at an intertopic distance._\n",
    "\n",
    "_If we look at Topic 2, the most probable words for the topic are infect, organ, virus, bacteria, species. I would guess that Topic 2 generally concerns pathology on a cellular level_\n",
    "\n",
    "_Topic 4 has top words like cause, association, common, use, treatment, and further down, words like patient, rash, pneumonia. I think this topic relates to disease and diagnosis._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "# Slide 27: Putting Eyes on It...\n",
    "![eye of sauron](images/eyeofsauron.jpg)\n",
    "[Matti Lyra - Evaluating Topic Models, Berlin 2017](https://www.youtube.com/watch?v=UkmIljRIG_M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "_So this is where we get back to the idea of a topic as a latent semantic signal within the corpus. When does a topic make sense and when does it not make sense? Also, how many topics, or what value of k is optimal, or even good enough?_\n",
    "\n",
    "_There's an excellent lecture on this problem, with Matti Lyra. Essentially, the metrics available for topic modeling are often pretty unhelpful, or worse, misleading. Constantly requiring human validation of topics is time consuming, costly and ongoing when you have a growing corpus._\n",
    "\n",
    "_I'm not going to get into the various interim metrics for validating the number of topics. I really recommend the Matti Lyra lecture for that._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "# Slide 28: Naive K Topic Selection\n",
    "[See this repo for output from evaluation plots](https://github.com/clownfragment/a-smarter-flashcard)\n",
    "\n",
    "**TFIDF Matrix**<br>\n",
    "$\n",
    "\\Downarrow\n",
    "$<br>\n",
    "**Dimensional Reduction using Singular Value Decomposition (SVD)**<br>\n",
    "&nbsp;Normalize output\n",
    "$\n",
    "\\Downarrow\n",
    "$<br>\n",
    "**KMeans with clusters** $\\mathbf k = [2, 3, ..., K]$<br>\n",
    "&nbsp;Can plot the gap statistic at every value of $k$<br>\n",
    "&nbsp;Can create elbow plots of the distortion at every value of $k$<br>\n",
    "$\n",
    "\\Downarrow\n",
    "$<br>\n",
    "**Project to 2D using tSNE**<br>\n",
    "&nbsp;Can measure entropy in the resulting matrix<br>\n",
    "&nbsp;Can quickly look at resulting similarity projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "_Instead, I'm going to show a naive method I piece-mealed, using dimensional reduction and kmeans, which I'll project to 2 dimensions using tSNE. I don't know if anyone is doing this, and I have not rigorously examined it, so please take it with a grain of salt._\n",
    "\n",
    "_Instead of validating the LDA model, I've been performing a sort of meta-evaluation using KMeans. If you're not familiar with KMeans, just think of it as a way to group similar rows of data into clusters. Since Kmeans starts to breaks down at high dimensions, curse of dimensionality, I use SVD to reduce the dimensions to 50 singular value components that represent the variance in the original TFIDF sparse matrix._\n",
    "\n",
    "_I build KMeans over and over using a range, each time assigning the clustering results as labels on the documents. Keep in mind that, unlike LDA, KMeans does not assign a topic distribution to the documents, it only assigns a single cluster value. The assumption being made here is that document similarity will be enough to discern an appropriate number of topics by way of choosing an optimal number of clusters. To decide on an appropriate number of clusters, we can use Elbow plots for the gap statistic or for the distortion._\n",
    "\n",
    "_I'm not going to show that here, but if you look at the repo from this slide, there are distortion plots for various sample sizes of the corpus and various values of K clusters. In fact, if you generate a corpus and want to run an LDA diagnostic on it, the scripts on the source repo should get you started. There's also a Flask-based recommender that you can plug LDA, NMF or any other other model commonly used for recommenders, with a few modifications._\n",
    "\n",
    "_Of the various types of diagnostic plots for LDA, I'm only going to show the tSNE plots today, because I think they're neat, visually understandable and they give a decent sense of using KMeans to choose the value of k topics._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "# Slide 29: About [SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition)\n",
    "![SVD image, from wikipedia](images/Singular_value_decomposition_visualisation.svg)\n",
    "- Common dimensionaly reduction technique\n",
    "- Reduce sparse TF matrix to 50 singular values\n",
    "    - Singular value matrix where singular values \n",
    "    are uncorrelated and collectively account for \n",
    "    xx% of the variance in the data \n",
    "- SKlearn's [PCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) uses full SVD under the hood\n",
    "- Using [truncatedSVD](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html) here to return a reduced, dense matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "_So let's talk about SVD, KMeans and tSNE really quick so that the tSNE plots make sense. I'm not going into what's under the hood with these, I'm just going to discuss their use in generating the plots. I also want to say that this procedure may have glaring flaws that I have yet to understand, especially in the assumption that meaningful distances will be maintained in the dimensional reduction for KMeans to operate correctly. I'm still reading on this. Anyway,_\n",
    "\n",
    "_Singular Value Decomposition is used here to reduce the dimensionality of the sparse matrix, since KMeans doesn't do well with high dimensions. The resulting matrix is composed of uncorrelated singular values, very much like principal components in PCA, that account for a high percentage of the variance in the original data. Since I'm not reconstructing the data, we're not particularly concerned with that percentage. We're essentially getting the top 50 uncorrelated singular values and normalizing them. This process is basically LSA, Latent Semantic Analysis, which is often used as another topic modeling engine along with NMF_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "# Slide 30: About [KMeans](https://en.wikipedia.org/wiki/K-means_clustering)\n",
    "![By Chire [GFDL (http://www.gnu.org/copyleft/fdl.html) or CC BY-SA 4.0  (https://creativecommons.org/licenses/by-sa/4.0)], from Wikimedia Commons](https://upload.wikimedia.org/wikipedia/commons/e/ea/K-means_convergence.gif)\n",
    "- partition $n$ observations into $k$ clusters\n",
    "    - Assign each observation to nearest centroid (closest mean)\n",
    "    - Calculate new centroids\n",
    "    - Convergence\n",
    "- Here, we're using sklearn's [KMeans](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) to generate cluster labels to be applied to the reduced document matrix so that we can add label colors to tSNE\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "_So now that dimensions are reduced, the 50 feature matrix is passed into KMeans. Our main use for KMeans is to generate cluster labels for the documents, which are then applied to the reduced document matrix._\n",
    "\n",
    "_In this 2d graphic, we're seeing a linear boundary, but remember we're dealing with a 50 dimensional space, so boundaries will be created from multiple hyperplanes, creating convex hulls around the clusters._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "# Slide 31: About [tSNE](http://jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf)\n",
    "![tSNE w/ KMeans, 3_clusters, 20000 docs](images/tSNE_wKMeans_SVD_3_clusters_19998_docs.png)\n",
    "\n",
    "- t-Distributed Stochastic Neighbor Embedding: [interactive visualization](https://distill.pub/2016/misread-tsne/)\n",
    "- Effective way to visualize structure in high-dimensional data at 2D or 3D\n",
    "    - Better at separation than most other [Manifold Learning methods](http://scikit-learn.org/stable/modules/manifold.html)\n",
    "- Through pairwise comparison\n",
    "    - Similar observations pull together\n",
    "    - Dissimilar observations push apart,\n",
    "- Tunable by `Perplexity`: guess about the number of close neighbors \n",
    "    - Using default values from Yellowbrick [TSNEVisualizer](http://www.scikit-yb.org/en/latest/api/text/tsne.html) for text\n",
    "- **X and Y are not meaningful here, nor is the intracluster distance!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "_The way I like to think of tSNE is as if you throw a bunch of paintballs up in the air and freeze time, then move similar paintballs close to other similar paintballs and further from dissimilar paintballs, then unfreeze time and let them all drop down onto a piece of paper on the ground, you'll get a 2dimensional representation of similarity. Sometimes it's informative to make a 3d representation of higher dimensional data, and tSNE is good for that as well._\n",
    "\n",
    "_It's important to realize that the axes are meaningless here and distance is meaningful only in the sense that similar items will bunch together. This aspect of tSNE bothers a lot of people, but we're only using to get a sense as whether there is underlying structure in high dimensional data._\n",
    "\n",
    "_I'm using a tool from Yellowbrick to accomplish the tSNE visualization, but you could build the plots with matplotlib and sklearn almost as easily if you want to tune the perplexity. Yellowbrick's version of tSNE is pretuned for text and gives a nice plot._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "# Slide 32: KMeans/tSNE Visualization on Growing Corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "# Slide X: LDA Advantages\n",
    "- Effective\n",
    "- Easy to understand output topic representation\n",
    "- Great visualization tools (pyLDAvis, yellowbrick, etc)\n",
    "- Can be applied to many types of problems, not just NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "# Slide X: LDA Disadvantages\n",
    "- Number of topics?\n",
    "- Growing Corpus Problem\n",
    "- Cannot Explicitly Capture Correlations between Topics\n",
    "- No preservation of word order in the documents unless we use n-grams and massively increase dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### Further Study:\n",
    "- [Source Repo for LDA Flashcard Analysis](https://github.com/clownfragment/a-smarter-flashcard/)\n",
    "- [Further LDA Notes that Accompany this Notebook](https://github.com/clownfragment/Discovering-Topics-in-a-Corpus-using-Latent-Dirichlet-Allocation/blob/master/lda_notes.ipynb)\n",
    "- Latent Dirichlet Allocation: Towards a Deeper Understanding, Colorado Reed\n",
    "    - http://obphio.us/pdfs/lda_tutorial.pdf\n",
    "- LDAvis: A method for Visualizing and interpreting topics\n",
    "    - https://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf\n",
    "- _Matti Lyra - Evaluating Topic Models_, Youtube, PyData, 2017 https://www.youtube.com/watch?v=UkmIljRIG_M\n",
    "\n",
    "- _Estimating the numbers of clusters in a data set via the gap statistic._ Tibshirani, R., Walther, G., and Hastie, T. (2001).  J. R. Statist. Soc. B, 63(2): 411-423. http://www.stanford.edu/~hastie/Papers/gap.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
