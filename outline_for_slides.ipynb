{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Outline for LDA Lecture @MSU ML Club\n",
    "- Friday 11am, Oct 12\n",
    "- It is the meeting room in the offices. AES 200k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Slide 1\n",
    "- Title: Discovering Topics in a Corpus using Latent Dirichlet Allocation\n",
    "![process high level view](images/pipeline.png)\n",
    "![logos](images/logos.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "- Hello, my name is Tovio Roberts and I'm here to talk about Latent Dirichlet Allocation, known as the other LDA, not to be confused with [Linear Discriminant Analysis](https://en.wikipedia.org/wiki/Linear_discriminant_analysis). \n",
    "\n",
    "- Before we get into LDA, \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Slide 2: About me:\n",
    "- Tovio Roberts\n",
    "    - Former Arabic major @Wayne State, Detroit\n",
    "        - lots of language study in a variety of languages\n",
    "    - Former CompSci mentor with cognitively diverse college students\n",
    "    - Galvanize DSI 64 alumnus\n",
    "    - Lead Technical Admissions Officer for the Data Science Immersive at Galvanize\n",
    "    - Data Specialist at Carbon Collective\n",
    "    - Former CompSci major @MSU\n",
    "        - Currently IDP in Machine Learning @MSU, ~Spring 2019?\n",
    "    - Exploring Masters programs in Computational Linguistics\n",
    "    \n",
    "- github/clownfragment\n",
    "- dataisartisdata.com\n",
    "- tovioroberts@gmail.com\n",
    "- linkedin.com/in/Tovio-Roberts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "- _A little bit about me..._\n",
    "\n",
    "- _I've studied languages quite a bit, former Arabic major._\n",
    "\n",
    "- _I currently work as the lead technical admissions officer for the Data Science Immersive at Galvanize and as the data specialist (whatever that means) for a small startup called Carbon Collective working on gamifying carbon footprint reduction._\n",
    "\n",
    "- _I attended the 3 month Data Science Immersive at Galvanize last Spring._\n",
    "\n",
    "- _I worked for about 5 years as a CompSci tutor and mentor with Cognitively Diverse college students, mostly with ASD, in a one on one format, assisting with executive function, designing specialized curriculums and study plans._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Slide 3: First, Let's Frame a Use Case\n",
    "![Consolidate Documents from different Departments](images/departments_to_corpus.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "- _So let's motivate LDA through a context._\n",
    "\n",
    "- _In a university, there are many departments. Let's say MSU, wants to digitize their study resources across all departments and create a document recommender system that will suggest materials based on documents similar to what a given student is studying. The administration plans on pooling thousands of digital documents into a single corpus._\n",
    "\n",
    "- _Notice here that the concept of a \"subject\" is defined by humans. Potentially, the documents could be given tags to facilitate searching. However, we're hoping to automate the process of document suggestion with little human-intervention._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Slide 4: Subjects vs Topics\n",
    "- **Subject**: human-projected category, ie., History, Biology, Data Science\n",
    "- **Topic**: a latent signal discerned within a corpus, often described by a list of words/stems that \"should make sense\" to a human observer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "- _I am going to posit that, in order to deliver all appropriate study materials from many different departments, we should abandon the human-projected subject associated with the document. We might want to do this because we care primarily about the content of the document.  For example, if we're discussing formal languages, we want to make sure that content from the Linguistics department that is similar to content from the Computer Science department, such as those describing the Chomsky hierarchy of formal grammars, be related regardless._\n",
    "\n",
    "- _We want the machine to do the work of discovering the distribution of latent topics in a given document._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Slide 5: What do We Feed to the Machine?\n",
    "![truckasaurus](images/truckasaurus.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "- _What do we feed to our machine?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Slide 6: Term Frequency (TF) Matrix\n",
    "![bag of words image](images/bow.png)\n",
    "- aka, Bag of Words/Term Frequency\n",
    "    - order of words in the document is not relevant\n",
    "    - sparse matrix \n",
    "- Requires an NLP cleaning pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "- _Theoretically, Latent Dirichlet Allocation requires a Term Frequency Matrix, though I've used TFIDF matrices with similar results to TF matrices. Keep in mind that the input matrix is rather wide, the length of a row being the length of the vocabulary of the entire corpus. In other words, there's a column for every distinct word found in every document in the input._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Slide 7: TF Matrix / Bag of Words\n",
    "\n",
    "| doc index | word0 | word1 | word2 | word3 |... | wordn\n",
    "|-----------|-------|-------|-------|-------|----|------\n",
    "|doc0       | 0     | 1     | 0     |  2    |... | 1\n",
    "|doc1       | 0     | 0     | 1     |  0    |... | 0 \n",
    "|doc2       | 1     | 2     | 0     |  0    |... | 0\n",
    "|doc3       | 1     | 0     | 0     |  1    |... | 1\n",
    "|...        | ...   | ...   | ...   | ...   |... | ...\n",
    "|docn       | 0     | 2     | 0     |  0    |... | 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "_So, every document in our input is represented by a row, and has a frequency value for every word in the vocabulary. \n",
    "Realistically , a given document will not contain most of the words in the vocabulary, thus this is a sparse matrix._\n",
    "\n",
    "_We arrive at a usable TF Matrix through constructing a Cleaning Pipeline. To give a sense of what this pipeline is doing, let's look at some real documents._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Slide 8: Let's Build a Corpus\n",
    "- Sample of 36000 documents (Anki flash cards) from 3 Human-Defined Subjects\n",
    "    - 12000 from History\n",
    "    - 12000 from Data Science/Statistics\n",
    "    - 12000 from Biology\n",
    "- Using 1-grams, for sake of dimensionality, though n-grams are possible\n",
    "- Remove words most/least common across documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "_As a proof of concept, let's build a corpus where the documents come from 3 distinct, human-defined subjects, History, Data Science, and Biology. I used flash card decks contributed to ankiweb_\n",
    "\n",
    "_We can imagine that a topic-modeling algorithm searching for 3 topics will likely distinguish three topics represented by words that seem to satisfy the three subjects._\n",
    "\n",
    "_I'm using 1-grams here for \"words,\" but there's no reason that you couldn't use any n-gram representation, it just massively increases the dimensions of the sparse matrix and wasn't at all necessary given the subject delineation of this corpus._\n",
    "\n",
    "_One last adjustment that I commit is to remove the words that appear in more than a certain percentage of documents, as well as words that only appear in less than a certain number of documents. This is similar to the automatic weighting you see in Term Frequency, Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Slide 9: Input Must be Cleaned\n",
    "![uncleaned input data](images/wordmap_data_uncleaned.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "_Here we see a wordcloud from just the Data Science cards. We can see a number of artifacts from html, javascript and LaTEX. My pipeline includes functions to strip code, numeric characters and LaTEX. I will likely rewrite my cleaning functions to keep the LaTEX in a translated form, as some cards only have LaTEX and the word 'derivative' would likely contribute to a document's topic distribution._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Slide 10: Features (words) should be Canonical\n",
    "- Stemming: reduce to a \"root\" form\n",
    "    - Eliminates tense\n",
    "- Could instead lemmatize for potentially more readable root word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "_In general when we use a TF or TFIDF matrix, we want to stem the words. Stemming strips different forms of a given word, such as the words \"complex\" and \"complexity\", to a single canonical form._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Slide 11: Cleaned/Stemmed Document\n",
    "![document before and after](images/doc_before_after.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "_This is a single document, before and after cleaning. The underlined words illustrate stemming, html has been removed, and everything is lowercased._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Slide 12: Data Science Cards, Cleaned and Stemmed\n",
    "![data science wordcloud](images/data_science_wordcloud.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "_Here we see a wordcloud of the cleaned and stemmed vocabulary from the DataScience flashcard deck. The larger the word, the more frequently it occurs in the corpus._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Slide 13: Biology Cards, Cleaned and Stemmed\n",
    "![biology wordcloud](images/biology_wordcloud.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "- _And the same for Biology_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Slide 14: History Cards, Cleaned and Stemmed\n",
    "![history wordcloud](images/history_wordcloud.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "_And History_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Slide 15: Cleaned Corpora Combined\n",
    "\n",
    "![Full cleaned corpus](images/full_corpus_cleaned.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "_And here we see a wordcloud for the entire corpus. We can see some remnants of markup and code, but I think we're doing ok._\n",
    "\n",
    "_I just want to take a moment and say that Data Cleaning takes a long time. In natural language processing, especially when using a bag of words or TFIDF representation of documents, cleaning is key as it provides better consistency and reduces dimensionality of your input._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Slide 16: Some Selected Word Counts\n",
    "![some selected word counts](images/some_selected_word_counts.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "_If we look at some counts of words from the different decks, we can get an idea of word relevance or salience to a given subject. It's very telling that the word \"distribution\" occurs 800 times in the Data Science deck, and less than 40 times in each of the other two decks. The word \"cell\" does not occur in History, and \"war\" only occurs in \"History\"._\n",
    "\n",
    "_Intuitively, we can start think of modeling topics as a process involving observation of word occurrences in documents as well relative to word occurences within the corpus._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Slide 17: What is Topic Modeling?\n",
    "- Unsupervised Learning method for finding latent semantic structures in a text or corpus\n",
    "- Common to assume that a given document is primarily \"about\" a given topic\n",
    "- Common to assume that a document clustered into a given topic will have a high occurrence of topic-salient words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "_That intuition is justified. Topic Modeling is the field within unsupervised learning dedicated to finding latent semantic structures within text and within corpora._\n",
    "\n",
    "_For our purposes, let's assume that a given document is primarily about a given topic and that that document will contain a high frequency of words representative of that topic._\n",
    "\n",
    "_Now that we have a prepared corpus and an understanding that our goal is to discern latent semantic structures in the corpus, we can more easily describe LDA._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Slide 18: Latent Dirichlet Allocation\n",
    "- 2002: David Blei, Andrew Ng, and Michael I. Jordan\n",
    "- \"LDA yields list of salient groups of related words over a collection, with documents represented as a mixture of these groups.\"\n",
    "    - A document is a probability distribution over topics\n",
    "    - A topic is a probability distribution over words\n",
    "- More Intuitive Interpretation than SVD or NMF\n",
    "    - ie., doc0 = {topic1: 50%, topic2: 25%,  topic3 : 25%}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "_Latent Dirichlet Allocation is one of the most common topic modeling algorithms, alongside more matrix-based techniques such as Singular Value Decomposition and Non-Negative Matrix Factorization._\n",
    "\n",
    "_\"LDA is capable of yielding a list of salient groups of related words, aka 'topics', over a collection, with documents represented as a mixture of these groups or topics.\"_\n",
    "\n",
    "_We can think of a document as a probability distribution over topics, and a topic as a probability distribution over words. Notice here, again, that we will assume that documents with similar topics will use similar groups of words. **Latent topics are discovered by discovering groups of words in the corpus that frequently occur together within documents**_ \n",
    "\n",
    "_What really discern LDA from other bag of words models, like Non-negative Matrix Factorization or Singular Value Decomposition, is that we're not really focused on the term frequencies, we are concerned with the distribution of words across topics._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Slide 18: LDA, a Simple Metaphor\n",
    "- you want to figure out peoples' (words') interests (topics) in a city based on where they hang out\n",
    "- assume that people go to specific places based on interests they share with other people who go to those places\n",
    "- assume we can associate people with interests and places with interests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "_Let's say you move to a new city and you want to figure out people's interests based on where they hang out. If you assume people go to specific places based on interests they share with other people, then we can assume that interests are represented by a cross section of people and that places draw certain people based on being intersections of interests._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Slide 19: ... Walking around town\n",
    "\n",
    "1. Choose a number of interests/topics, k=3\n",
    "2. Guess as to what people are indicative of the given interests\n",
    "3. Guess the interests that draw a specific person to a specific place\n",
    "4. Over and over again\n",
    "    - For each place and person\n",
    "        - update likelihood of the guesses based on other people in that space\n",
    " \n",
    "- The guesses will get better \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "_You decide that there are 3 interests people may have that would lead them to a certain place._\n",
    "\n",
    "_So you take a first walk around town and record guesses as to which people represent certain interests, and you guess percentages of the 3 interests that might draw people to a given place._\n",
    "\n",
    "_Since you're new in town, you're very likely to make a lot of bad guesses. However, you're planning on repeating your walk every day for a few years, updating the guesses in your notebook on every walk._\n",
    "\n",
    "_You'll start recognizing that certain people appear in certain places, in the company of other people with similar interests. Eventually, you find that you can describe interests in terms of the people are into those interests._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Slide 20: LDA, Restated\n",
    "[source](http://obphio.us/pdfs/lda_tutorial.pdf)\n",
    "- Given a Corpus of documents, LDA learns the topic representation in each document and the word distribution of each topic\n",
    "- LDA identifies the topics that are likely to have generated the corpus \n",
    "- uses a sparse Dirichlet prior:\n",
    "    - samples from a simplex so the k topics add to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "_So let's restate LDA really quick then describe a less metaphorical view of the LDA procedure._\n",
    "\n",
    "_Given a corpus of documents, LDA learns the topic representation in each document and the word distribution of each topic. The assumption is that a distribution of topics is used to generate each document within the corpus._\n",
    "\n",
    "_We use a sparse Dirichlet prior to seed the LDA model on it's initial random assignments of each word in a given document to each of the topics, as well as the mix of topic probabilities for each document in the corpus._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "# Slide 21: LDA, High-Level View\n",
    "[For more in-depth view of LDA, see the accompanying notebook](https://github.com/clownfragment/Discovering-Topics-in-a-Corpus-using-Latent-Dirichlet-Allocation/blob/master/lda_notes.ipynb)\n",
    "1. Randomly assign each word in each document to one of the _k_ topics\n",
    "2. For each document $d$:\n",
    "    - Assume all topic assignments except for current one to be correct\n",
    "    - Calculate 2 proportions\n",
    "        1. Proportion of words in document $d$ that are currently assigned to topic $t = p(\\text{topic } t \\mid \\text{document }d)$\n",
    "        2. Proportion of assignments to topic $t$ over all documents that came from this word $w = p(\\text{word }w \\mid \\text{topic }t)$\n",
    "    - multiply the proportions and assign $w$ a new topic based on that probability.\n",
    "        - $p(\\text{topic } t \\mid \\text{document }d) \\times p(\\text{word } w \\mid \\text{topic }t)$\n",
    "            - probability that topic $t$ generated word $w$ in a given document\n",
    "3. Reach steady state and return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "_So here is a slightly deeper way to understand what LDA is doing. If you look at the github repo for this presentation, there's more in-depth material there, along with links to further explanations if you want to go even deeper._\n",
    "\n",
    "_Let's walk through this really quick before taking a look at some results_\n",
    "\n",
    "_Initially, we randomly assign each word in each document to a topic. We then look at each document in turn and calculate the proportions of words in that document that are currently assigned to a given topic. We also calculate the proportion of assignments of that topic over all documents that have that word. We multiply those proportions and assign a new topic to the document based on that product._\n",
    "\n",
    "_We do this over and over again for every document, either for a maximum number of epochs or until we don't see a significant update in the probability distributions._\n",
    "\n",
    "_This is effectively the Expectation Maximization algorithm, but using the Dirichlet distribution instead of a Gaussian. I have some pseudocode for the EM on the repo as well._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "# Slide 22: Results on the Flashcard Corpus\n",
    "[Repo for LDA on flashcard corpus](https://github.com/clownfragment/a-smarter-flashcard)\n",
    "\n",
    "* [Interactive LDA on Term Freq Matrix of Full Corpus](https://rawgit.com/clownfragment/a-smarter-flashcard/master/images/all_count_vect_topics.html)\n",
    "* [Interactive LDA on TF-IDF Matrix of Full Corpus](https://rawgit.com/clownfragment/a-smarter-flashcard/master/images/all_tfidf_vect_topics.html)\n",
    "* [Interactive LDA on Term Freq Matrix of Biology Subcorpus](https://rawgit.com/clownfragment/a-smarter-flashcard/master/images/bio_count_vect_topics.html)\n",
    "* [Interactive LDA on TF-IDF Matrix of Biology Subcorpus](https://rawgit.com/clownfragment/a-smarter-flashcard/master/images/bio_tfidf_vect_topics.html)\n",
    "* [Interactive LDA on Term Freq Matrix of Data Science Subcorpus](https://rawgit.com/clownfragment/a-smarter-flashcard/master/images/datascience_count_vect_topics.html)\n",
    "* [Interactive LDA on TF-IDF Matrix of Data Science Subcorpus](https://rawgit.com/clownfragment/a-smarter-flashcard/master/images/datascience_tfidf_vect_topics.html)\n",
    "* [Interactive LDA on Term Freq Matrix of History Subcorpus](https://rawgit.com/clownfragment/a-smarter-flashcard/master/images/his_count_vect_topics.html)\n",
    "* [Interactive LDA on TF-IDF Matrix of History Subcorpus](https://rawgit.com/clownfragment/a-smarter-flashcard/master/images/his_tfidf_vect_topics.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "_So let's look at some results of the LDA model. I used the Gensim version instead of the sklearn version of LDA. To visualize the topic distributions, I'm using a pretty amazing tool called PyLDAvis._\n",
    "\n",
    "**click on Term Freq of Full Corpus**\n",
    "\n",
    "_The Left side of the Plot is constructed using MDS (MultiDimensional Scaling) where PCA is projecting to a 2-dimensional space, and Jensen-Shannon divergence gives distance measurement between topic distributions of words. The x and y axes don't mean much here aside from framing those relative distances._ \n",
    "\n",
    "_The size of the topic bubbles indicates the prevalence of a given topic, which is not the number of documents in a given topic cluster, but the occurrence of a given topic across the entire corpus._\n",
    "\n",
    "\n",
    "**Click on Topics 1, 2 and 3**\n",
    "\n",
    "_Let's look at each topic in turn and look at words in each topic ranked by their topic-specific probability, or $\\lambda$ set to 1. Here, $\\lambda$, controls a log likelihood weighting between term probabilities within the topic and the discernment power of terms within the topic, also known as the lift. By setting $\\lambda$ to one, we are seeing words that have high occurrence in documents when the mix of that topic is high._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "# Slide 23: What does Topic 1 Reflect?\n",
    "![data science terms](images/data_science_terms.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "_So we can make a guess as to what subject topic 1 seems to be reflecting. Remember, we pooled three subjects, Data Science, Biology and History. The high occurrence words for topic 1 are data, model, variable, value, distribution, feature... what does that sound like?  Data Science_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "# Slide 24: What does Topic 2 Reflect?\n",
    "![history terms](images/history_terms.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "_Topic 2 has high occurrence words like state, war, empire, people, and power.  What does that sound like?  History.  Notice here that the word \"amask\" is high-occurring as well. That is a formatting remnant that was not removed during cleaning, as is the word \"style\" which occurs a little further down._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "# Slide 25: What does Topic 3 Reflect?\n",
    "![biology terms](images/biology_terms.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "_Topic 3 is pretty clearly representative of cards from the Biology flashcard set._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "# Slide 26: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### Further Study:\n",
    "- [Further LDA Notes that Accompany this Notebook](https://github.com/clownfragment/Discovering-Topics-in-a-Corpus-using-Latent-Dirichlet-Allocation/blob/master/lda_notes.ipynb)\n",
    "- Latent Dirichlet Allocation: Towards a Deeper Understanding, Colorado Reed\n",
    "    - http://obphio.us/pdfs/lda_tutorial.pdf\n",
    "- LDAvis: A method for Visualizing and interpreting topics\n",
    "    - https://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
