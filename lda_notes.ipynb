{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Notes on LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "- Compiled from Resources listed at the bottom\n",
    "- Tovio Roberts\n",
    "- 2018, October\n",
    "- tovioroberts@gmail.com\n",
    "    - please contact me to suggest edits, as this is meant to be a learning resource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "- **topic**: probability distribution over a collection of words, a thematic summary\n",
    "- **topic model**: formal statistical relationship betw a group of observed and latent (unknown) random variables that specifies a probabilistic procedure to generate the topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Generative Model\n",
    "- **dataset**: collection of _D_ documents\n",
    "- **document**: a collection of words\n",
    "    - in LDA, contains multiple topics\n",
    "    - in LDA, order of words is not not maintained (b.o.w.)\n",
    "- **vocabulary**: all unique words in the corpus\n",
    "    - _V_ : number of terms in the corpus\n",
    "- assume we know _K_ topic distributions in the dataset\n",
    "    - _K_ multinomials containing _V_ elements each\n",
    "    - $\\beta_1$ : represents the multinomial for the $i$th topic\n",
    "        - size of $\\beta_1$ is $V: \\mid \\beta_1 \\mid = V$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### How LDA assumes documents are created:\n",
    "1. Determine number of words in a document\n",
    "2. Choose a topic mixture for the document over a fixed set of topics (25% Topic 0, 45% Topic 1, 30% Topic 2)\n",
    "3. Generate the words in the document:\n",
    "    - a. first pick a topic based on the document's multinomial distribution from step 2\n",
    "    - b. pick a word based on the topic's multinomial distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### LDA Statistical Inference Procedure for learning the topics and topic distributions\n",
    "- Given a Corpus of documents, LDA learns the topic representation in each document and the word distribution of each topic\n",
    "    - LDA identifies the topics that are likely to have generated the corpus\n",
    "\n",
    "#### High-level view of the LDA process\n",
    "- **Essentially updating assignments of words with evolving model of how documents are created**\n",
    "1. Randomly assign each word in each document to one of the _k_ topics\n",
    "2. For each document $d$:\n",
    "    - Assume all topic assignments except for current one to be correct\n",
    "    - Calculate 2 proportions\n",
    "        1. Proportion of words in document $d$ that are currently assigned to topic $t = p(\\text{topic } t \\mid \\text{document }d)$\n",
    "        2. Proportion of assignments to topic $t$ over all documents that came from this word $w = p(\\text{word }w \\mid \\text{topic }t)$\n",
    "    - multiply the proportions and assign $w$ a new topic based on that probability.\n",
    "        - $p(\\text{topic } t \\mid \\text{document }d) \\times p(\\text{word } w \\mid \\text{topic }t)$\n",
    "            - probability that topic $t$ generated word $w$ in a given document\n",
    "3. Reach steady state and return parameters\n",
    "    - topic mixtures of each of the documents\n",
    "    - underlying word distributions for topics\n",
    "\n",
    "\n",
    "#### Mid-level view of the LDA process\n",
    "- For each document $d$\n",
    "    - (a) draw a topic distribution, $\\theta_d \\sim Dir(\\alpha)$, where $Dir(\\cdot)$ is a draw from a _uniform_ Dirichlet distribution with scaling parameter $\\alpha$\n",
    "        - this a draw from a _k_ dimensional Dirichlet distribution which returns a _k_ dimensional multinomial $\\theta$ where the _k_ values sum to 1\n",
    "    - (b) for each word in the document:\n",
    "        - (i) Draw a specific topic $z_{d,n} \\sim multi(\\theta_d)$ where $multi(\\cdot)$ is a multinomial\n",
    "        - (ii) Draw a word $w_{d,n} \\sim \\beta_{z_{d,n}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### Prob Density of k-dimensional multinomial $\\theta$ in LDA\n",
    "$$\n",
    "p(\\theta \\mid \\alpha) = \\frac{\\Gamma (\\sum_{i=1}^k \\alpha_i)}{\\prod_{i=1}^k \\Gamma(\\alpha_i)}\\prod_{i=1}^k \\theta_i^{\\alpha_i - 1}\n",
    "$$\n",
    "- $\\theta$ is normalized ensuring it lies on a $(k-1)$ dimensional simplex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### LDA Graphical Model\n",
    "- using plate notation, a way of visually representing the dependencies among the model parameters\n",
    "\n",
    "![LDA Graphical Model](images\\lda_graphical_model.png)\n",
    "\n",
    "- $M$ : denotes lg rectangle, reps total number of documents in the corpus\n",
    "- $N$ : smaller rectangle, total number of words in a document\n",
    "- $\\theta_d$ : topic distribution for document $d$ of $M$\n",
    "- $w$ : a word\n",
    "    - $w_{d,n}$ : a specific word\n",
    "    - $w^v$ : the $v$th word in the vocabulary\n",
    "    - $\\mathbf w$ : a document/vector of words, $w = (w_1, w_2, ..., w_N)$\n",
    "- $\\mathbf z$ : vector of topics from which $w$ draws, used to notate each topic, which is assigned to each word, making each document a mixture of these topics\n",
    "    - $z_{d,n}$ : topic for the $n$th word in document $d$ \n",
    "- Dirichlet Priors: $\\alpha$ and $\\beta$\n",
    "    - $\\alpha$ : parameter of Dirichlet prior on the per-document topic distributions\n",
    "        - high $\\alpha$ indicates that each document is likely to contain a mixture of most of the topics, not 1 or 2 in particular\n",
    "            - low $\\alpha$ indicates that only a couple or few topics will be dominant\n",
    "        - $\\alpha = (\\alpha_1, \\alpha_2, ..., \\alpha_k)$\n",
    "        - all elems of $\\alpha$ will usually be the same and can be rep'd in code by a single number\n",
    "    - $\\beta$ : $k \\times V$ word-probability matrix\n",
    "        - parameter of the Dirichlet prior on the per-topic word distribution\n",
    "            - high $\\beta$ indicates that each topic will contain a mixture of most of the words, low indicates each topic contains a mixture of just a few of the words\n",
    "        - row : topic\n",
    "        - column : term\n",
    "        - $\\beta_{ij} = p(w^j = 1 \\mid z^i = 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### Central Inferential Problem : Determine the Posterior Distr of the latent variables given the document\n",
    "$$\n",
    "p(\\theta, \\mathbf z \\mid \\mathbf w, \\alpha, \\beta) = \\frac{p(\\theta, \\mathbf z, \\mathbf w \\mid \\alpha, \\beta)}{p(\\mathbf w \\mid \\alpha, \\beta)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Downarrow\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{decompose numerator into hierarchy by examining graphical model}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Downarrow\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(\\theta, \\mathbf z, \\mathbf w \\mid \\alpha, \\beta) = p(\\mathbf w \\mid \\mathbf z, \\beta)p(\\mathbf z \\mid \\theta)p(\\theta \\mid \\alpha)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{where } p(\\mathbf w \\mid \\mathbf z, \\beta) \\\\\n",
    "\\text{ represents the probability of observing a document with } N \\\\\n",
    "\\text{ words given the topic vector of length } N \\text{ that assigns a topic to each word from the } \\\\ \n",
    "k \\times V \\text{ probability matrix } \\beta \\\\\n",
    "\\text{thus we can decompose this into each individual} \\\\\n",
    "\\text{word probability and multiply them together:}\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(\\mathbf w \\mid \\mathbf z, \\beta) = \\prod_{n=1}^N \\beta_{z_n, w_n}\n",
    "$$\n",
    "\n",
    "<hr>\n",
    "\n",
    "$$\n",
    "p(\\mathbf z \\mid \\theta) \\text{ is trivial:} \\\\\n",
    "p(z_n \\mid \\theta) = \\theta_i, \\text{ such that } z_n^i = 1 \\\\\n",
    "\\text{ as }\\theta \\text{ is a multinomial}\n",
    "$$\n",
    "\n",
    "<hr>\n",
    "\n",
    "$$\n",
    "\\text{recall: } \\\\\n",
    "p(\\theta \\mid \\alpha) = \\frac{\\Gamma (\\sum_{i=1}^k \\alpha_i)}{\\prod_{i=1}^k \\Gamma(\\alpha_i)}\\prod_{i=1}^k \\theta_i^{\\alpha_i - 1}\n",
    "$$\n",
    "\n",
    "<hr>\n",
    "\n",
    "$$\n",
    "\\text{using } \\theta_{z_n} \\text{ to rep component of } \\theta \\text{ chosen for } z_n : \\\\\n",
    "p(\\theta, \\mathbf z, \\mathbf w \\mid \\alpha, \\beta) = \\Biggl( \\frac{\\Gamma ( \\sum_{i=1}^k \\alpha_i)}{\\prod_{i=1}^k \\Gamma (\\alpha_i)}\\prod_{i=1}^k \\theta_i^{\\alpha_i - 1} \\Biggr) \\prod_{n=1}^N \\beta_{z_n, w_n} \\theta_{z_n}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Downarrow\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{rephrased using word-indexing supercript }w^v \\text{ and words used as exponent}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Downarrow\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(\\theta, \\mathbf z, \\mathbf w \\mid \\alpha, \\beta) = \\Biggl( \\frac{\\Gamma ( \\sum_{i=1}^k \\alpha_i)}{\\prod_{i=1}^k \\Gamma (\\alpha_i)}\\prod_{i=1}^k \\theta_i^{\\alpha_i - 1} \\Biggr) \\prod_{n=1}^N \\prod_{i=1}^k \\prod_{j=1}^V ( \\theta_i \\beta_{i,j})^{w_n^j z_n^i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Downarrow\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{marginalize over } \\theta \\text{ and } z \\text{ in order to obtain the denominator (the evidence)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Downarrow\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(\\mathbf w \\mid \\alpha, \\beta) = \\frac{\\Gamma ( \\sum_{i=1}^k \\alpha_i)}{\\prod_{i=1}^k \\Gamma (\\alpha_i)} \\int \\Biggl( \\prod_{i=1}^k \\theta_i^{\\alpha_i - 1} \\Biggr) \\Biggl( \\prod_{n=1}^N \\sum_{i=1}^k \\prod_{j=1}^V (\\theta_i \\beta_{ij})^{w_{n}^j} \\Biggr) d \\theta\n",
    "$$\n",
    "- this final expression cannot be computed, as the inability to separate $\\theta$ and $\\beta$ prevents computing the log of the function, so use variational inference techniques in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### Variational Inference for LDA\n",
    "- `variational inference` : use simpler, convex distribution that obtains an adjustable lower bound on the log likelihood of the actual distribution\n",
    "- `variational parameterss` : describe the family of simpler distributions used to determine a lower bound on the log likelohood, optimized to create the tightest possible lower bound\n",
    "\n",
    "##### _Drop coupling betw $\\theta$ and $\\beta$ to make inference possible_\n",
    "\n",
    "![LDA Graphical Model](images\\lda_graphical_model.png)\n",
    "$$\n",
    "\\Downarrow\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{drop some edges and nodes from fig 2 to obtain simplified graphical model}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Downarrow\n",
    "$$\n",
    "\n",
    "![Variational LDA Graphical Model](images\\variational_lda_graphic.png)\n",
    "\n",
    "$$\n",
    "\\text{posterior for each document, from the variational distribution} \\\\\n",
    "q(\\theta, \\mathbf z \\mid \\gamma, \\phi) = q(\\theta \\mid \\gamma) \\prod_{n=1}^N q(z_n \\mid \\phi_n)\n",
    "$$\n",
    "\n",
    "- **Find an optimal lower bound on log likelihood results using minimization of the Kellback-Leibler (KL) divergence betw the variational distribution and the actual posterior distribution:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Using Expectation Maximization Algorithm on Variational Distribution\n",
    "\n",
    "- **Expectation Maximization**\n",
    "    - use EM to discover parameters of the prob distribution, automatically discover all parameters for the _K_ sources\n",
    "    - mixture model used to perform `soft clustering`\n",
    "        - `hard clustering`: no overlap of clusters\n",
    "        - `soft clustering`: clusters may overlap\n",
    "    - each cluster corresponds to a prob distribution\n",
    "    - Unlike KMeans, does not assign to hard cluster\n",
    "    - iterates until convergence\n",
    "\n",
    "- **EM Steps**: E-Step, M-Step\n",
    "    - Estimate $\\beta$ and $\\alpha$ assuming we know $\\phi$ $\\gamma$\n",
    "        - $\\alpha$ : parameter of Dirichlet distribution\n",
    "        - $\\beta$ : $k \\times V$ word-probability matrix\n",
    "    - `E-Step` : detm log likelihood of the complete data, estimate hidden parameters $\\alpha$ and $\\beta$ by finding the optimal values of the variational parameters $\\gamma_d^*$ and $\\phi_d^*$ for every document in the corpus\n",
    "    - `M-Step` : Maximize lower bound on the log likelihood $l(\\alpha, \\beta) = \\sum_{d=1}^M log p(\\mathbf w_d \\mid \\alpha, \\beta)$ with respect to $\\alpha$ and $\\beta$\n",
    "        - corresponds to finding maximum likelihood estimates with expected sufficient statistics for each document\n",
    "- **Normalization/Scaling**\n",
    "    - normalize all $\\beta_i$ to sum to one\n",
    "    - use linear-scaling Newton-Rhapson algorithm to determine optimal $\\alpha$, updates carried out in log-space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "#### Pseudocode for Variational Expectation-Maximization LDA, [From Reed 2012](http://obphio.us/pdfs/lda_tutorial.pdf)\n",
    "\n",
    "\n",
    "##### Input:\n",
    "Number of topics $K$<br>\n",
    "Corpus with $M$ documents and $N_d$ words in document $d$<br>\n",
    "\n",
    "##### Output:\n",
    "Model parameters: $\\beta, \\theta, z$<br>\n",
    "\n",
    "##### Setup\n",
    "initialize $\\phi_{ni}^0$ := 1/$k$ for all $i$ in $k$ and $n$ in $N_d$<br>\n",
    "initialize $\\gamma_i$ := $\\alpha_i + N/k$ for all $i$ in $k$<br>\n",
    "initialize $\\alpha$ := $50/k$<br>\n",
    "initialize $\\beta_{ij}$ := $0$ for all $i$ in $k$ and $j$ in $V$<br>\n",
    "\n",
    "##### E-Step: Determine $\\phi$ and $\\gamma$ and compute expected log likelihood\n",
    "**for** $d = 1$ **to** $M$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**repeat**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**for** $n = 1$ **to** $N_d$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **for** $i=1$ **to** $K$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\phi_{dni}^{t+1}$ := $\\beta_{iw_n} exp(\\Psi(\\gamma_{di}^t))$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **endfor**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; normalize $\\phi_{dni}^{t+1}$ to sum to 1<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**endfor**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\gamma^{t+1}$ := $\\alpha + \\sum_{n=1}^N \\phi_{dn}^{t+1}$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**until** convergence of $\\phi_d$ and $\\gamma_d$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;loglikelihood := loglikelihood + $L(\\gamma, \\phi; \\alpha, \\beta)$\n",
    "**endfor**<br>\n",
    "\n",
    "##### M-Step: maximize the log likelohood of the variational distribution\n",
    "**for** $d=1$ **to M**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**for** $i=1$ **to K**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**for** $j = 1$ **to** $V$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\beta_{ij}$ := $\\phi_{dni} w_{dnj}$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**endfor**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;normalize $\\beta_i$ to sum to 1<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**endfor**<br>\n",
    "**endfor**<br>\n",
    "estimate $\\alpha$ via $log(\\alpha^{t+1}) = log(\\alpha^t) - \\frac{\\frac{dL}{d\\alpha}}{\\frac{d^2L}{d\\alpha^2} \\alpha + \\frac{dL}{d\\alpha}}$<br>\n",
    "**if** loglikelihood converged **then**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;return parameters<br>\n",
    "**else**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;go back to E-step<br>\n",
    "**endif**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Further Resources:\n",
    "##### LDA\n",
    "- [_Latent Dirichlet Allocation: Towards a Deeper Understanding_](http://obphio.us/pdfs/lda_tutorial.pdf), Colorado Reed, Jan 2012\n",
    "- _LDA Algorithm Description_ Youtube, Scott Sullivan, Mar 2017, https://www.youtube.com/watch?v=DWJYZq_fQ2A\n",
    "- _LDA Topic Models_, Youtube, Andrius Knispelis, July 2016, https://www.youtube.com/watch?v=3mHy4OSyRf0\n",
    "- _Introduction to the Dirichlet Distribution and Related Processes_, B . Friqyik, A. Kapila, M. Gupta, Dec 2010, http://mayagupta.org/publications/FrigyikKapilaGuptaIntroToDirichlet.pdf\n",
    "\n",
    "##### EM Algorithm\n",
    "- _Pattern Recognition and Machine Learning_, Ch 9, Christopher M. Bishop, 2006\n",
    "- _EM algorithm: how it works_, Victor Lavrenko, 2014, https://www.youtube.com/watch?v=REypj2sy_5U\n",
    "- _Expectation Maximization (EM algorithm) intuitive explanation_, Youtube, pp, https://www.youtube.com/watch?v=j-3z97CYD44\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
